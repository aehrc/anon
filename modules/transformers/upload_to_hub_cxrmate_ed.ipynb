{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cxrmate_ed.modelling_cxrmate_ed import MIMICIVEDCXRMultimodalModel\n",
    "from cxrmate_ed.modelling_uniformer import MultiUniFormerWithProjectionHead\n",
    "from cxrmate_ed.configuration_uniformer import UniFormerWithProjectionHeadConfig\n",
    "from cxrmate_ed.records import EDCXRSubjectRecords\n",
    "from cxrmate_ed.tables import NUM_ED_CXR_TOKEN_TYPE_IDS\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import warnings\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.40.2', '2.1.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub checkpoint name:\n",
    "hub_ckpt_name = 'aehrc/cxrmate-ed'\n",
    "ckpt_zoo_dir = '/datasets/work/hb-mlaifsp-mm/work/checkpoints'\n",
    "database_path = '/scratch3/nic261/database/mimic_iv_duckdb_rev_d.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths:\n",
    "ckpt_path = '/datasets/work/hb-mlaifsp-mm/work/repositories/transmodal/cxrmate2/experiments/cxrmate2/cxrmate2/005_scst_cxrbert_bertscore/trial_3/epoch=22-step=54924-val_findings_bertscore_f1=0.443515.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state dict with depreciated keys:\n",
    "state_dict = torch.load(ckpt_path, map_location=torch.device('cpu'))['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-to-decoder instance:\n",
    "MIMICIVEDCXRMultimodalModel.register_for_auto_class(\"AutoModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description, Special token, Index\n",
      "bos_token, [BOS], 1\n",
      "eos_token, [EOS], 2\n",
      "unk_token, [UNK], 0\n",
      "sep_token, [SEP], 3\n",
      "pad_token, [PAD], 4\n",
      "cls_token, [BOS], 1\n",
      "mask_token, [MASK], 5\n"
     ]
    }
   ],
   "source": [
    "records = EDCXRSubjectRecords(database_path=database_path, time_delta_map=lambda x: 1 / math.sqrt(x + 1))\n",
    "\n",
    "records.ed_module_tables = {k: records.ed_module_tables[k] for k in ['edstays', 'triage', 'vitalsign']}\n",
    "records.mimic_cxr_tables = {k: records.mimic_cxr_tables[k] for k in ['mimic_cxr_sectioned']}\n",
    "records.mimic_cxr_tables['mimic_cxr_sectioned'].text_columns = ['indication', 'history']\n",
    "\n",
    "index_value_encoder_config = {}\n",
    "for k, v in (records.ed_module_tables | records.mimic_cxr_tables).items():\n",
    "    if v.load and (v.value_columns or v.index_columns):\n",
    "        index_value_encoder_config[k] = v.total_indices\n",
    "\n",
    "# Decoder tokenizer:\n",
    "encoder_decoder_ckpt_name = f'{ckpt_zoo_dir}/mimic_iv_tokenizers/bpe_cxr_findings_impression_indication_history_ed_medrecon_vitalsign_triage'\n",
    "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(encoder_decoder_ckpt_name)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Print the special tokens:\n",
    "print('Description, Special token, Index')\n",
    "for k, v in tokenizer.special_tokens_map.items():\n",
    "    if k != 'additional_special_tokens':\n",
    "        print(f'{k}, {v}, {getattr(tokenizer, k + \"_id\")}')\n",
    "    else:\n",
    "        for i, j in zip(tokenizer.additional_special_tokens, tokenizer.additional_special_tokens_ids):\n",
    "            print(f'additional_special_token, {i}, {j}')\n",
    "\n",
    "# Decoder config:\n",
    "config_decoder = transformers.LlamaConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=768,\n",
    "    intermediate_size=3072,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    max_position_embeddings=2048,\n",
    ")\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.index_value_encoder_config = index_value_encoder_config\n",
    "config_decoder.index_value_encoder_intermediate_size = 2048\n",
    "config_decoder.ed_module_columns = [f'{k}_{i}' for k, v in records.ed_module_tables.items() for i in v.text_columns]\n",
    "config_decoder.mimic_cxr_columns = [i for _, v in records.mimic_cxr_tables.items() for i in v.text_columns]\n",
    "config_decoder.token_type_to_token_type_id = records.token_type_to_token_type_id\n",
    "config_decoder.num_token_types = NUM_ED_CXR_TOKEN_TYPE_IDS\n",
    "config_decoder.include_time_delta = True\n",
    "config_decoder.time_delta_monotonic_inversion = True\n",
    "config_decoder.zero_time_delta_value = records.compute_time_delta(\n",
    "    datetime.datetime.fromtimestamp(0),\n",
    "    datetime.datetime.fromtimestamp(0), \n",
    "    to_tensor=False,\n",
    ")\n",
    "config_decoder.add_time_deltas = True\n",
    "\n",
    "# Section embedding identifiers (for report):\n",
    "config_decoder.section_ids = [\n",
    "    records.token_type_to_token_type_id['findings'], \n",
    "    records.token_type_to_token_type_id['impression'], \n",
    "]\n",
    "\n",
    "# Add set token identifiers in decoder's config:\n",
    "config_decoder.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Encoder config:\n",
    "config_encoder = UniFormerWithProjectionHeadConfig(\n",
    "    projection_size=config_decoder.hidden_size,\n",
    ")\n",
    "encoder_ckpt_name = 'uniformer_base_tl_384'\n",
    "\n",
    "# Encoder-to-decoder model:\n",
    "config = transformers.VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "config.decoder.add_cross_attention = False\n",
    "encoder_decoder = MIMICIVEDCXRMultimodalModel(\n",
    "    config=config, \n",
    "    DefaultEncoderClass=MultiUniFormerWithProjectionHead,\n",
    "    DefaultDecoderClass=transformers.LlamaForCausalLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(state_dict.keys()):\n",
    "    if 'encoder_decoder.' in key:\n",
    "        state_dict[key.replace('encoder_decoder.', '')] = state_dict.pop(key)\n",
    "    else:\n",
    "        warnings.warn(f'Key not found: {key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load renamed state dict:\n",
    "encoder_decoder.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model:\n",
    "save_path = '/scratch3/nic261/checkpoints/cxrmate_ed'\n",
    "encoder_decoder.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /scratch3/nic261/.cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Hub login:\n",
    "from huggingface_hub import login\n",
    "\n",
    "with open('/home/nic261/hf_token.txt', 'r') as f:\n",
    "    token = f.readline()\n",
    "login(token=token[:-1])\n",
    "del token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aehrc/cxrmate-ed/commit/d7831418c6fed23bc77f78eade260f758fb61d56', commit_message='Upload tokenizer', commit_description='', oid='d7831418c6fed23bc77f78eade260f758fb61d56', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to hub:\n",
    "encoder_decoder.push_to_hub(hub_ckpt_name)\n",
    "tokenizer.push_to_hub(hub_ckpt_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxrmate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
